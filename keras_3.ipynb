{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maureenwidjaja/HW0/blob/main/keras_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "operational-party"
      },
      "source": [
        "# Text Classification and Word Embedding\n",
        "\n",
        "In this set of notes, we'll discuss the problem of *text classification*. Text classification is a common problem in which we aim to classify pieces of text into different categories. These categories might be about:\n",
        "\n",
        "- **Subject matter**: news, fashion, finance?\n",
        "- **Emotional valence**: Happy or sad? Excited or calm? This particular class of questions is so important that it has its own name: *sentiment analysis.*\n",
        "- **Automated content moderation**: Facebook comment / reddit thread abuse / harassment? promoting violence? spam?\n",
        "- **AI-Generated** : ChatGPT or not?\n",
        "\n",
        "Today, we will do simple example of subject matter classification. We'll also do some sentiment analysis later.\n",
        "\n",
        "\n",
        "### Bias in Natural Language Processing\n",
        "\n",
        "Like all other machine learning algorithms, natural language algorithms naturally inherit the biases of both the data on which they are trained and the choices made by researchers in training the algorithms. A sub-theme of this set of lectures is the need to carefully check our model outputs so that we can understand the impact of each of these.\n",
        "\n",
        "#### Supplementary Materials\n",
        "\n",
        "- [Term-document matrices](https://nbviewer.jupyter.org/github/PhilChodrow/PIC16A/blob/master/content/NLP/NLP_1.ipynb).\n",
        "- [Sentiment analysis](https://nbviewer.jupyter.org/github/PhilChodrow/PIC16A/blob/master/content/NLP/NLP_3.ipynb).\n",
        "\n",
        "#### Related Resources\n",
        "\n",
        "- This set of lecture notes is partially based on this [official tutorial](https://www.tensorflow.org/tutorials/keras/text_classification).\n"
      ],
      "id": "operational-party"
    },
    {
      "cell_type": "markdown",
      "source": [
        "One thing we will _not_ do today is using the JAX backend. While JAX can be faster in many tasks, it does not support strings. We will use TensorFlow backend instead."
      ],
      "metadata": {
        "id": "T3WCPXGpYkIL"
      },
      "id": "T3WCPXGpYkIL"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\""
      ],
      "metadata": {
        "id": "hdxXiH-vYibM"
      },
      "id": "hdxXiH-vYibM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "documentary-chambers"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import keras\n",
        "from keras import layers, losses\n",
        "from keras.layers import TextVectorization\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# for embedding viz\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "pio.templates.default = \"plotly_white\"\n",
        "from keras import models"
      ],
      "id": "documentary-chambers"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "above-youth"
      },
      "source": [
        "For this example, we are going to use a data set containing headlines from a large number of different news articles on the website [HuffPost](https://www.huffpost.com/) retrieved [from Kaggle](https://www.kaggle.com/rmisra/news-category-dataset)."
      ],
      "id": "above-youth"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "desirable-roads"
      },
      "outputs": [],
      "source": [
        "url = \"https://raw.githubusercontent.com/pic16b-ucla/25W/master/datasets/news/News_Category_Dataset_v2.json\"\n",
        "df  = pd.read_json(url, lines=True)\n",
        "df  = df[[\"category\", \"headline\"]]"
      ],
      "id": "desirable-roads"
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "dxWUfca4OQj9"
      },
      "id": "dxWUfca4OQj9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "Z5693pnSV0iy"
      },
      "id": "Z5693pnSV0iy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "interstate-bubble"
      },
      "source": [
        "There are over 200,000 headlines listed here, along with the category in which they appeared on the website."
      ],
      "id": "interstate-bubble"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fossil-acrobat"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ],
      "id": "fossil-acrobat"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alpine-apache"
      },
      "source": [
        "Our task will be to teach an algorithm to classify headlines by predicting the category based on the text of the headline.\n",
        "\n",
        "Training a model on this much text data can require a lot of time, so we are going to simplify the problem a little bit, by reducing the number of categories. Let's take a look at which categories we have:"
      ],
      "id": "alpine-apache"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "governing-source"
      },
      "outputs": [],
      "source": [
        "df.groupby(\"category\").size()"
      ],
      "id": "governing-source"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plastic-colon"
      },
      "source": [
        "Some of these categories are a little odd:\n",
        "\n",
        "- \"Women\"?\n",
        "- \"Weird News\"?\n",
        "- What's the difference between \"Style,\" \"Style & Beauty,\" and \"Taste\"? ).\n",
        "- \"Parenting\" vs. \"Parents\"?\n",
        "- Etc?...\n",
        "\n",
        "Well, there are definitely some questions here! Let's just choose a few categories, and discard the rest:"
      ],
      "id": "plastic-colon"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dietary-planning"
      },
      "outputs": [],
      "source": [
        "categories = [\"STYLE\", \"SCIENCE\", \"TECH\"]\n",
        "\n",
        "df = df[df[\"category\"].apply(lambda x: x in categories)]\n",
        "df.head()"
      ],
      "id": "dietary-planning"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blank-living"
      },
      "source": [
        "Next, we'll use a `LabelEncoder` to transform the `category` column into integers.\n",
        "\n",
        "**Note**: I couldn't find a way that I was satisfied with to do this in TensorFlow/JAX/PyTorch, but if you know a smooth way, let me know!"
      ],
      "id": "blank-living"
    },
    {
      "cell_type": "code",
      "source": [
        "pd.__version__"
      ],
      "metadata": {
        "id": "GdFZOQ0bvqWh"
      },
      "id": "GdFZOQ0bvqWh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FO6oRmRAZN4"
      },
      "outputs": [],
      "source": [
        "le = LabelEncoder()\n",
        "df[\"category_encoded\"] = le.fit_transform(df[\"category\"])\n",
        "# df.loc[:, \"category_encoded\"] = le.fit_transform(df[\"category\"]) -- same warning\n",
        "# df.loc[:, (\"category_encoded\",)] = le.fit_transform(df[\"category\"]) -- no warning\n",
        "df.head()"
      ],
      "id": "0FO6oRmRAZN4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "I think `SettingWithCopyWarning` on Pandas 2.2.2 here is a false positive. If you really want to avoid this warning, there is a way to do it shown above; somewhat unintuitive.\n"
      ],
      "metadata": {
        "id": "_knWjRI6AZN5"
      },
      "id": "_knWjRI6AZN5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sc6tF2uTSA7T"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ],
      "id": "Sc6tF2uTSA7T"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "optional-single"
      },
      "source": [
        "Later, we'll be able to remember which integers correspond to which classes using the `classes_` attribute of the encoder."
      ],
      "id": "optional-single"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amino-drama"
      },
      "outputs": [],
      "source": [
        "le.classes_"
      ],
      "id": "amino-drama"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stock-proposal"
      },
      "source": [
        "We're left with a much smaller number of rows, which will be much easier to work with.\n",
        "\n",
        "#### So Far....\n",
        "\n",
        "...we have accessed our data, examined the categories available, and taken a subset of the data corresponding to just three categories."
      ],
      "id": "stock-proposal"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "continued-continuity"
      },
      "source": [
        "## TensorFlow Datasets\n",
        "\n",
        "Next, we are going create a TensorFlow `Dataset` from our data frame. While we often talk colloquially about \"data sets\", TensorFlow has a special `Dataset` class with a number of convenient capabilities. Use of `Dataset`s is generally optional, but can make it significantly easier to stay organized when writing data pipelines. The `Dataset` class also includes functionality for a wide-variety of data input scenarios, including situations in which the data should be read in chunks-at-a-time from disk. JAX (not used today) is focused on performance optimization, and does not come with a nice data loading functionalities. You can find TensorFlow `Dataset`s and PyTorch `DataLoader` helpful in those situations.\n",
        "\n",
        "The `Dataset` class is useful for all kinds of problems, not just text classification problems. Learn more about it [here](https://www.tensorflow.org/guide/data).\n",
        "\n",
        "We'll make a dataset with the predictor data (the headline) and target data (the category) separated out."
      ],
      "id": "continued-continuity"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "excess-leisure"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "data = tf.data.Dataset.from_tensor_slices((df[\"headline\"], df[\"category_encoded\"]))"
      ],
      "id": "excess-leisure"
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "id": "8WEM-4BDYfUP"
      },
      "id": "8WEM-4BDYfUP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "insured-establishment"
      },
      "outputs": [],
      "source": [
        "for headline, category in data.take(5):\n",
        "    print(headline)\n",
        "    print(category)\n",
        "    print(\"\")"
      ],
      "id": "insured-establishment"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fixed-letters"
      },
      "source": [
        "Now we'll perform a train-test split. We'll also take out a small validation set."
      ],
      "id": "fixed-letters"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egyptian-yahoo"
      },
      "outputs": [],
      "source": [
        "data = data.shuffle(len(data), reshuffle_each_iteration=False)"
      ],
      "id": "egyptian-yahoo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usual-entity"
      },
      "outputs": [],
      "source": [
        "train_size = int(0.7*len(data))\n",
        "val_size   = int(0.1*len(data))\n",
        "\n",
        "train = data.take(train_size)\n",
        "val   = data.skip(train_size).take(val_size)\n",
        "test  = data.skip(train_size + val_size)"
      ],
      "id": "usual-entity"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "generous-nickname"
      },
      "outputs": [],
      "source": [
        "len(train), len(val), len(test)"
      ],
      "id": "generous-nickname"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "devoted-poster"
      },
      "source": [
        "#### So far....\n",
        "\n",
        "...we have created a special TensorFlow `Dataset` and split it into training, validation, and testing sets."
      ],
      "id": "devoted-poster"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acquired-diagram"
      },
      "source": [
        "## Standardization and Vectorization\n",
        "\n",
        "*Standardization* refers to the act of taking a some text that's \"messy\" in some way and making it less messy. Common standardizations include:\n",
        "\n",
        "- Removing capitals.\n",
        "- Removing punctuation.\n",
        "- Removing HTML elements or other non-semantic content.\n",
        "\n",
        "In this standardization, we convert all text to lowercase and remove punctuation."
      ],
      "id": "acquired-diagram"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lonely-delicious"
      },
      "outputs": [],
      "source": [
        "def standardization(input_data):\n",
        "    lowercase = tf.strings.lower(input_data)\n",
        "    no_punctuation = tf.strings.regex_replace(lowercase,\n",
        "                                  '[%s]' % re.escape(string.punctuation),'')\n",
        "    return no_punctuation"
      ],
      "id": "lonely-delicious"
    },
    {
      "cell_type": "code",
      "source": [
        "print(re.escape(string.punctuation))"
      ],
      "metadata": {
        "id": "eQDJiQwbaLwb"
      },
      "id": "eQDJiQwbaLwb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "polish-painting"
      },
      "source": [
        "*Vectorization* refers to the process of representing text as a vector (array, tensor). There are multiple ways to carry out vectorization. For example, forming a *term-document matrix*, as demonstrated in the optional review lecture notes, is one way to form vectors from text. Here, we'll use a different approach: we'll replace each word by its *frequency rank* in the data. For example, the headline\n",
        "\n",
        "> Poll: Penguins Best Bird\n",
        "\n",
        "might have representation\n",
        "\n",
        "```[708, 1567, 89, 632].```\n",
        "\n",
        "This means that \"poll\" is the 708th most common word in the data set, \"penguins\" is the 1567 most common word in the data set, and so on.  \n",
        "\n",
        "For technical details on how TensorFlow carries out the vectorization, check [the docs](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization). Note that we pass the standardization from above as an argument to the vectorization layer.  "
      ],
      "id": "polish-painting"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "after-receipt"
      },
      "outputs": [],
      "source": [
        "# only the top distinct words will be tracked\n",
        "max_tokens = 2000\n",
        "\n",
        "# each headline will be a vector of length 25\n",
        "sequence_length = 25\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize=standardization,\n",
        "    max_tokens=max_tokens, # only consider this many words\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)"
      ],
      "id": "after-receipt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "assumed-frederick"
      },
      "source": [
        "We need to *adapt* the vectorization layer to the headlines. In the adaptation process, the vectorization layer learns what words are common in the headlines."
      ],
      "id": "assumed-frederick"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bridal-reserve"
      },
      "outputs": [],
      "source": [
        "headlines = train.map(lambda x, y: x)\n",
        "vectorize_layer.adapt(headlines)"
      ],
      "id": "bridal-reserve"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "absent-mississippi"
      },
      "source": [
        "Now we're ready to vectorize each of the data sets. To do so, we define a helper function that operates on our Datasets. Note that our Dataset consists of a bunch of tuples of the form (headline, category) for each data observation. Our helper function therefore accepts and returns two variables.\n",
        "\n",
        "**Note**: because we adapted the vectorization layer to the training data, not the validation or testing data, we aren't \"cheating\" by propagating information from the validation or testing data prior to the training step."
      ],
      "id": "absent-mississippi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "infrared-agent"
      },
      "outputs": [],
      "source": [
        "def vectorize_headline(text, label):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    return vectorize_layer(text), [label]\n",
        "\n",
        "train_vec = train.map(vectorize_headline)\n",
        "val_vec   = val.map(vectorize_headline)\n",
        "test_vec  = test.map(vectorize_headline)"
      ],
      "id": "infrared-agent"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-EEurJ6zsc5"
      },
      "outputs": [],
      "source": [
        "vectorize_layer.get_vocabulary()"
      ],
      "id": "u-EEurJ6zsc5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "electric-azerbaijan"
      },
      "source": [
        "Let's take a look at a vectorized piece of text."
      ],
      "id": "electric-azerbaijan"
    },
    {
      "cell_type": "code",
      "source": [
        "for h, c in train.take(3):\n",
        "  print(h)\n",
        "  print(vectorize_layer(h))"
      ],
      "metadata": {
        "id": "v-7p4-PFcXsY"
      },
      "id": "v-7p4-PFcXsY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "worth-rebate"
      },
      "source": [
        "#### So far...\n",
        "\n",
        "...we have finally prepared our data! We have represented each of our headlines as numerical vectors, which are something that TensorFlow is able to understand."
      ],
      "id": "worth-rebate"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conservative-visibility"
      },
      "source": [
        "## Modeling\n",
        "\n",
        "Phew, that was a lot of data preparation! That's kind of how it is in the world of machine learning: so much of the effort goes into ensuring that your data is correctly formatted and represented.\n",
        "\n",
        "Let's now construct a simple model out of some layers. This model is going to have a few new components.\n",
        "\n",
        "The most interesting of these, which we are going to come back to, is the `Embedding` layer. Because we're going to come back to it, let's give it a name!"
      ],
      "id": "conservative-visibility"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "empty-european"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential([\n",
        "  layers.Input((sequence_length,)),\n",
        "  layers.Embedding(max_tokens, output_dim = 3, name=\"embedding\"),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.GlobalAveragePooling1D(),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.Dense(30),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.Dense(30),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.Dense(len(categories))\n",
        "  ]\n",
        ")"
      ],
      "id": "empty-european"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piano-prior"
      },
      "source": [
        "We use `Dropout` layers in the model. What these layers do is disable (\"drop out\") a fixed percentage of the units in each layer, *but only during training.* This turns out to be a good way to reduce the risk of overfitting."
      ],
      "id": "piano-prior"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "broke-minority"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ],
      "id": "broke-minority"
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "QsB_Gtl-nmLH"
      },
      "id": "QsB_Gtl-nmLH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "corrected-difference"
      },
      "source": [
        "It's time to fit our model."
      ],
      "id": "corrected-difference"
    },
    {
      "cell_type": "markdown",
      "source": [
        "But wait! Throughout deep learning lectures, we have seen that just minimizing the loss would result in overfitting. How do we avoid it?\n",
        "\n",
        "We have a simple idea of __early stopping__. When the performance on the validation set no longer improves for several epochs, we stop training there. Then, we have a model performing reasonably on both the training set and the validation set."
      ],
      "metadata": {
        "id": "pa_BiJ5djtTO"
      },
      "id": "pa_BiJ5djtTO"
    },
    {
      "cell_type": "code",
      "source": [
        "callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)"
      ],
      "metadata": {
        "id": "pqEBuvuNjrgp"
      },
      "id": "pqEBuvuNjrgp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "assumed-secretariat"
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_vec, epochs = 20, validation_data = val_vec,\n",
        "                    callbacks=[callback])"
      ],
      "id": "assumed-secretariat"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "forbidden-traveler"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.plot(history.history[\"accuracy\"], label = \"training\")\n",
        "plt.plot(history.history[\"val_accuracy\"], label = \"validation\")\n",
        "plt.gca().set(xlabel = \"epoch\", ylabel = \"accuracy\")\n",
        "plt.legend()"
      ],
      "id": "forbidden-traveler"
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.plot(history.history[\"loss\"], label = \"training\")\n",
        "plt.plot(history.history[\"val_loss\"], label = \"validation\")\n",
        "plt.gca().set(xlabel = \"epoch\", ylabel = \"loss\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "lMIoV0uKthVS"
      },
      "id": "lMIoV0uKthVS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "burning-board"
      },
      "source": [
        "# Predictions on Unseen Data\n",
        "\n",
        "Let's check our model performance on unseen data."
      ],
      "id": "burning-board"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "canadian-serbia"
      },
      "outputs": [],
      "source": [
        "model.evaluate(test_vec)"
      ],
      "id": "canadian-serbia"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "comic-fleece"
      },
      "source": [
        "Not bad! We're able to correctly classify the category of a given news headline on HuffPost 85-86% of the time, at least when we are choosing between the three categories that we selected earlier."
      ],
      "id": "comic-fleece"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suburban-porter"
      },
      "source": [
        "### So far...\n",
        "\n",
        "...we have trained our model and evaluated it on unseen data, obtaining reasonable results."
      ],
      "id": "suburban-porter"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "compatible-condition"
      },
      "source": [
        "# Embeddings\n",
        "\n",
        "A *word embedding* refers to a representation of a word in a vector space. Each word is assigned an individual vector. The general aim of a word embedding is to create a representation such that words with related meanings are close to each other in a vector space, while words with different meanings are farther apart. One usually hopes for the *directions* connecting words to be meaningful as well. Here's a nice diagram illustrating some of the general concepts:\n",
        "\n",
        "![](https://miro.medium.com/max/1838/1*OEmWDt4eztOcm5pr2QbxfA.png)\n",
        "\n",
        "*Image credit: [Towards Data Science](https://towardsdatascience.com/creating-word-embeddings-coding-the-word2vec-algorithm-in-python-using-deep-learning-b337d0ba17a8)*\n",
        "\n",
        "Word embeddings are often produced as intermediate stages in many machine learning algorithms. In fact, we already made one -- it's the `Embedding` layer at the base of our model. Let's take a look at the embedding layer to see how our own model represents words in a vector space.\n",
        "\n",
        "We chose to create a 3-dimensional embedding when constructing our model. This is fine for today, but state-of-the-art embeddings will typically have a much higher number of dimensions.  For example, the [Embedding Projector demo](http://projector.tensorflow.org/) supplied by TensorFlow uses a default dimension of 200."
      ],
      "id": "compatible-condition"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "light-verse"
      },
      "outputs": [],
      "source": [
        "weights = model.get_layer('embedding').get_weights()[0] # get the weights from the embedding layer\n",
        "vocab = vectorize_layer.get_vocabulary()                # get the vocabulary from our data prep for later"
      ],
      "id": "light-verse"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JSJNHbvBSia"
      },
      "outputs": [],
      "source": [
        "weights = model.get_layer('embedding').get_weights()[0]"
      ],
      "id": "9JSJNHbvBSia"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "appropriate-crossing"
      },
      "outputs": [],
      "source": [
        "weights.shape"
      ],
      "id": "appropriate-crossing"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "early-render"
      },
      "source": [
        "The collection of weights is 3-dimensional. For plotting in 2 dimensions, we have several choices for how to reduce the data to a 2d representation. A very simple and standard approach is our friend, principal component analysis (PCA)."
      ],
      "id": "early-render"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "local-vintage"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "weights = pca.fit_transform(weights)"
      ],
      "id": "local-vintage"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "characteristic-cradle"
      },
      "source": [
        "Now we'll make a data frame from our results:"
      ],
      "id": "characteristic-cradle"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clear-visiting"
      },
      "outputs": [],
      "source": [
        "embedding_df = pd.DataFrame({\n",
        "    'word' : vocab,\n",
        "    'x0'   : weights[:,0],\n",
        "    'x1'   : weights[:,1]\n",
        "})\n",
        "embedding_df"
      ],
      "id": "clear-visiting"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "impaired-tunnel"
      },
      "source": [
        "Ready to plot! Note that the embedding appear to be \"stretched out\" in three directions, with one direction corresponding to each of the three categories (tech, style, science)."
      ],
      "id": "impaired-tunnel"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "horizontal-auction",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "fig = px.scatter(embedding_df,\n",
        "                 x = \"x0\",\n",
        "                 y = \"x1\",\n",
        "                 size = list(np.ones(len(embedding_df))),\n",
        "                 size_max = 3,\n",
        "                 hover_name = \"word\")\n",
        "\n",
        "fig.show()"
      ],
      "id": "horizontal-auction"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "theoretical-sweden"
      },
      "source": [
        "Cool, we made a word embedding! This embedding seems to have learned some reasonable associations. For example, we see that words like \"Mars\", \"NASA\", and \"space\" are relatively close to each other. So are \"Facebook\", \"Google\", and \"Apple\", as well as \"fashion\", \"dress\", and \"style.\"\n",
        "\n",
        "## Bias in Language Models\n",
        "\n",
        "Whenever we create a machine learning model that might conceivably have impact on the thoughts or actions of human beings, we have a responsibility to understand the limitations and biases of that model. Biases can enter into machine learning models through several routes, including the data used as well as choices made by the modeler along the way. For example, in our case:\n",
        "\n",
        "1. **Data**: we used data from a popular news source.\n",
        "2. **Modeler choice**: we only used data corresponding to a certain subset of labels.\n",
        "\n",
        "With these considerations in mind, let's see what kinds of words our model associates with female and male genders."
      ],
      "id": "theoretical-sweden"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "golden-bookmark"
      },
      "outputs": [],
      "source": [
        "feminine = [\"she\", \"her\", \"woman\"]\n",
        "masculine = [\"he\", \"him\", \"man\"]\n",
        "\n",
        "highlight_1 = [\"strong\", \"powerful\", \"smart\",     \"thinking\"]\n",
        "highlight_2 = [\"hot\",    \"sexy\",     \"beautiful\", \"shopping\"]\n",
        "\n",
        "def gender_mapper(x):\n",
        "    if x in feminine:\n",
        "        return 1\n",
        "    elif x in masculine:\n",
        "        return 4\n",
        "    elif x in highlight_1:\n",
        "        return 3\n",
        "    elif x in highlight_2:\n",
        "        return 2\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "embedding_df[\"highlight\"] = embedding_df[\"word\"].apply(gender_mapper)\n",
        "embedding_df[\"size\"]      = np.array(1.0 + 50*(embedding_df[\"highlight\"] > 0))"
      ],
      "id": "golden-bookmark"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "social-motorcycle"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "\n",
        "fig = px.scatter(embedding_df,\n",
        "                 x = \"x0\",\n",
        "                 y = \"x1\",\n",
        "                 color = \"highlight\",\n",
        "                 size = list(embedding_df[\"size\"]),\n",
        "                 size_max = 10,\n",
        "                 hover_name = \"word\")\n",
        "\n",
        "fig.show()"
      ],
      "id": "social-motorcycle"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chemical-thinking"
      },
      "source": [
        "Our text classification model's word embedding is unambiguously sexist.\n",
        "\n",
        "- Words like \"hot\", \"sexy\", and \"shopping\" are more closely located to feminine words like \"she\", \"her\", and \"woman\".\n",
        "- Words like \"strong\", \"smart\", and \"thinking\" are more closely located to masculine words like \"he\", \"him\", and \"man\".\n",
        "\n",
        "Where did these biases come from?\n",
        "\n",
        "- The primary source is the data itself: HuffPost headlines in certain categories can be highly gendered, and the \"Style\" category is an example of this.\n",
        "- A secondary source is the choices that I made as a modeler. In particular, I intentionally chose categories that would emphasize biases in the data and make them easy to visualize.\n",
        "\n",
        "While I could have made different choices and obtained different results, this episode highlights a fundamental set of questions usually underexamined in contemporary machine learning:\n",
        "\n",
        "- What biases are built into my data source?\n",
        "- How do my choices about which data to use influence the biases present in my model?\n",
        "\n",
        "For more on the topic of bias in language models, you may wish to read the now-infamous paper by Emily Bender, Angelina McMillan-Major, Timnt Gebru, and \"Shmargret Shmitchell\" (Margret Mitchell), \"[On the Dangers of Stochastic Parrots](https://faculty.washington.edu/ebender/papers/Stochastic_Parrots.pdf).\" This is the paper that ultimately led to the firing of the final two authors by Google in late 2020 and early 2021."
      ],
      "id": "chemical-thinking"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BnmgRdDUj4aW"
      },
      "id": "BnmgRdDUj4aW",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}